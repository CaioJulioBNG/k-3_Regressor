{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85fea2b0-bdf9-4bd6-a37e-9b63701827b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000108 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1711\n",
      "[LightGBM] [Info] Number of data points in the train set: 1314, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 0.751142\n",
      "RMSE do CatBoost: 0.3464\n",
      "RMSE do XGBoost: 0.3602\n",
      "RMSE do LightGBM: 0.3451\n",
      "RMSE do Modelo Meta: 0.3378\n",
      "Blending conclu√≠do! Resultados salvos em submission.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Carregar datasets\n",
    "data_treino = pd.read_csv('/home/caio/github/k-3/data/train.csv')\n",
    "data_teste = pd.read_csv('/home/caio/github/k-3/data/test.csv')\n",
    "\n",
    "# Definir features e target\n",
    "X = data_treino.drop(columns=['rainfall'])  # Substitua pelo nome real da coluna alvo\n",
    "y = data_treino['rainfall']\n",
    "X_test = data_teste  # Apenas features\n",
    "\n",
    "# Dividir treino em treino principal (60%) e valida√ß√£o (40%)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Inicializar modelos base\n",
    "modelo_catboost = CatBoostRegressor(verbose=0, random_state=42)\n",
    "modelo_xgboost = XGBRegressor(n_estimators=100, random_state=42)\n",
    "modelo_lightgbm = LGBMRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Treinar modelos base no treino principal\n",
    "modelo_catboost.fit(X_train, y_train)\n",
    "modelo_xgboost.fit(X_train, y_train)\n",
    "modelo_lightgbm.fit(X_train, y_train)\n",
    "\n",
    "# Obter previs√µes dos modelos base no conjunto de valida√ß√£o\n",
    "preds_catboost = modelo_catboost.predict(X_valid)\n",
    "preds_xgboost = modelo_xgboost.predict(X_valid)\n",
    "preds_lightgbm = modelo_lightgbm.predict(X_valid)\n",
    "\n",
    "# Calcular RMSE para cada modelo individual\n",
    "rmse_catboost = np.sqrt(mean_squared_error(y_valid, preds_catboost))\n",
    "rmse_xgboost = np.sqrt(mean_squared_error(y_valid, preds_xgboost))\n",
    "rmse_lightgbm = np.sqrt(mean_squared_error(y_valid, preds_lightgbm))\n",
    "\n",
    "# Criar novo conjunto de treino para o modelo meta\n",
    "X_meta_train = np.column_stack([preds_catboost, preds_xgboost, preds_lightgbm])\n",
    "y_meta_train = y_valid\n",
    "\n",
    "# Treinar modelo meta (Regress√£o Ridge)\n",
    "modelo_meta = Ridge()\n",
    "modelo_meta.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Fazer previs√µes no conjunto de valida√ß√£o usando o modelo meta\n",
    "y_valid_pred = modelo_meta.predict(X_meta_train)\n",
    "\n",
    "# Calcular RMSE do modelo meta\n",
    "rmse_meta = np.sqrt(mean_squared_error(y_meta_train, y_valid_pred))\n",
    "\n",
    "# Exibir RMSEs\n",
    "print(f\"RMSE do CatBoost: {rmse_catboost:.4f}\")\n",
    "print(f\"RMSE do XGBoost: {rmse_xgboost:.4f}\")\n",
    "print(f\"RMSE do LightGBM: {rmse_lightgbm:.4f}\")\n",
    "print(f\"RMSE do Modelo Meta: {rmse_meta:.4f}\")\n",
    "\n",
    "# Obter previs√µes dos modelos base no conjunto de teste\n",
    "preds_catboost_test = modelo_catboost.predict(X_test)\n",
    "preds_xgboost_test = modelo_xgboost.predict(X_test)\n",
    "preds_lightgbm_test = modelo_lightgbm.predict(X_test)\n",
    "\n",
    "# Criar novo conjunto de teste para o modelo meta\n",
    "X_meta_test = np.column_stack([preds_catboost_test, preds_xgboost_test, preds_lightgbm_test])\n",
    "\n",
    "# Fazer previs√£o final com o modelo meta\n",
    "y_pred_final = modelo_meta.predict(X_meta_test)\n",
    "\n",
    "# Salvar resultados\n",
    "output = pd.DataFrame({'Id': data_teste['id'], 'rainfall': y_pred_final})\n",
    "output.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Blending conclu√≠do! Resultados salvos em submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "838bc1ae-07c1-469f-b5c4-e52ecbd1fc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000115 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1711\n",
      "[LightGBM] [Info] Number of data points in the train set: 1314, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 0.751142\n",
      "RMSE do CatBoost: 0.3464\n",
      "RMSE do XGBoost: 0.3602\n",
      "RMSE do LightGBM: 0.3451\n",
      "RMSE do KNN: 0.3773\n",
      "RMSE do Modelo Meta: 0.3360\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nKNeighborsRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m preds_xgboost_test \u001b[38;5;241m=\u001b[39m modelo_xgboost\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     71\u001b[0m preds_lightgbm_test \u001b[38;5;241m=\u001b[39m modelo_lightgbm\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m---> 72\u001b[0m preds_knn_test \u001b[38;5;241m=\u001b[39m modelo_knn\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Criar novo conjunto de teste para o modelo meta\u001b[39;00m\n\u001b[1;32m     75\u001b[0m X_meta_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcolumn_stack([preds_catboost_test, preds_xgboost_test, preds_lightgbm_test, preds_knn_test])\n",
      "File \u001b[0;32m~/Arquivos/conda/envs/ML/lib/python3.11/site-packages/sklearn/neighbors/_regression.py:243\u001b[0m, in \u001b[0;36mKNeighborsRegressor.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict the target for the provided data.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m    Target values.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# In that case, we do not need the distances to perform\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m# the weighting so we do not compute them.\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m     neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkneighbors(X, return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    244\u001b[0m     neigh_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Arquivos/conda/envs/ML/lib/python3.11/site-packages/sklearn/neighbors/_base.py:838\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    836\u001b[0m         X \u001b[38;5;241m=\u001b[39m _check_precomputed(X)\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 838\u001b[0m         X \u001b[38;5;241m=\u001b[39m validate_data(\n\u001b[1;32m    839\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    840\u001b[0m             X,\n\u001b[1;32m    841\u001b[0m             ensure_all_finite\u001b[38;5;241m=\u001b[39mensure_all_finite,\n\u001b[1;32m    842\u001b[0m             accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    843\u001b[0m             reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    844\u001b[0m             order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    845\u001b[0m         )\n\u001b[1;32m    847\u001b[0m n_samples_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_fit_\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_neighbors \u001b[38;5;241m>\u001b[39m n_samples_fit:\n",
      "File \u001b[0;32m~/Arquivos/conda/envs/ML/lib/python3.11/site-packages/sklearn/utils/validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/Arquivos/conda/envs/ML/lib/python3.11/site-packages/sklearn/utils/validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[0;32m-> 1107\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1108\u001b[0m         array,\n\u001b[1;32m   1109\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m   1110\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m   1111\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mensure_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1112\u001b[0m     )\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/Arquivos/conda/envs/ML/lib/python3.11/site-packages/sklearn/utils/validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    121\u001b[0m     X,\n\u001b[1;32m    122\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[1;32m    123\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[1;32m    124\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[1;32m    125\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    126\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    127\u001b[0m )\n",
      "File \u001b[0;32m~/Arquivos/conda/envs/ML/lib/python3.11/site-packages/sklearn/utils/validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nKNeighborsRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Carregar datasets\n",
    "data_treino = pd.read_csv('/home/caio/github/k-3/data/train.csv')\n",
    "data_teste = pd.read_csv('/home/caio/github/k-3/data/test.csv')\n",
    "\n",
    "# Definir features e target\n",
    "X = data_treino.drop(columns=['rainfall'])  # Substitua pelo nome real da coluna alvo\n",
    "y = data_treino['rainfall']\n",
    "X_test = data_teste  # Apenas features\n",
    "\n",
    "# Dividir treino em treino principal (60%) e valida√ß√£o (40%)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Inicializar modelos base\n",
    "modelo_catboost = CatBoostRegressor(verbose=0, random_state=42)\n",
    "modelo_xgboost = XGBRegressor(n_estimators=100, random_state=42)\n",
    "modelo_lightgbm = LGBMRegressor(n_estimators=100, random_state=42)\n",
    "modelo_knn = KNeighborsRegressor(n_neighbors=5)  # KNN com 5 vizinhos\n",
    "\n",
    "# Treinar modelos base no treino principal\n",
    "modelo_catboost.fit(X_train, y_train)\n",
    "modelo_xgboost.fit(X_train, y_train)\n",
    "modelo_lightgbm.fit(X_train, y_train)\n",
    "modelo_knn.fit(X_train, y_train)\n",
    "\n",
    "# Obter previs√µes dos modelos base no conjunto de valida√ß√£o\n",
    "preds_catboost = modelo_catboost.predict(X_valid)\n",
    "preds_xgboost = modelo_xgboost.predict(X_valid)\n",
    "preds_lightgbm = modelo_lightgbm.predict(X_valid)\n",
    "preds_knn = modelo_knn.predict(X_valid)\n",
    "\n",
    "# Calcular RMSE para cada modelo individual\n",
    "rmse_catboost = np.sqrt(mean_squared_error(y_valid, preds_catboost))\n",
    "rmse_xgboost = np.sqrt(mean_squared_error(y_valid, preds_xgboost))\n",
    "rmse_lightgbm = np.sqrt(mean_squared_error(y_valid, preds_lightgbm))\n",
    "rmse_knn = np.sqrt(mean_squared_error(y_valid, preds_knn))\n",
    "\n",
    "# Criar novo conjunto de treino para o modelo meta\n",
    "X_meta_train = np.column_stack([preds_catboost, preds_xgboost, preds_lightgbm, preds_knn])\n",
    "y_meta_train = y_valid\n",
    "\n",
    "# Treinar modelo meta (Regress√£o Ridge)\n",
    "modelo_meta = Ridge()\n",
    "modelo_meta.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Fazer previs√µes no conjunto de valida√ß√£o usando o modelo meta\n",
    "y_valid_pred = modelo_meta.predict(X_meta_train)\n",
    "\n",
    "# Calcular RMSE do modelo meta\n",
    "rmse_meta = np.sqrt(mean_squared_error(y_meta_train, y_valid_pred))\n",
    "\n",
    "# Exibir RMSEs\n",
    "print(f\"RMSE do CatBoost: {rmse_catboost:.4f}\")\n",
    "print(f\"RMSE do XGBoost: {rmse_xgboost:.4f}\")\n",
    "print(f\"RMSE do LightGBM: {rmse_lightgbm:.4f}\")\n",
    "print(f\"RMSE do KNN: {rmse_knn:.4f}\")\n",
    "print(f\"RMSE do Modelo Meta: {rmse_meta:.4f}\")\n",
    "\n",
    "# Obter previs√µes dos modelos base no conjunto de teste\n",
    "preds_catboost_test = modelo_catboost.predict(X_test)\n",
    "preds_xgboost_test = modelo_xgboost.predict(X_test)\n",
    "preds_lightgbm_test = modelo_lightgbm.predict(X_test)\n",
    "preds_knn_test = modelo_knn.predict(X_test)\n",
    "\n",
    "# Criar novo conjunto de teste para o modelo meta\n",
    "X_meta_test = np.column_stack([preds_catboost_test, preds_xgboost_test, preds_lightgbm_test, preds_knn_test])\n",
    "\n",
    "# Fazer previs√£o final com o modelo meta\n",
    "y_pred_final = modelo_meta.predict(X_meta_test)\n",
    "\n",
    "# Salvar resultados\n",
    "output = pd.DataFrame({'Id': data_teste['id'], 'rainfall': y_pred_final})\n",
    "output.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Blending conclu√≠do! Resultados salvos em submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "059cb69a-3c5b-4225-bd5f-d45bd1e8d5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000110 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1711\n",
      "[LightGBM] [Info] Number of data points in the train set: 1314, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 0.751142\n",
      "RMSE do CatBoost: 0.3464\n",
      "RMSE do XGBoost: 0.3602\n",
      "RMSE do LightGBM: 0.3451\n",
      "RMSE do KNN: 0.3773\n",
      "RMSE do RandomForest: 0.3398\n",
      "RMSE do ExtraTrees: 0.3376\n",
      "RMSE do GradientBoosting: 0.3373\n",
      "RMSE do DecisionTree: 0.4558\n",
      "RMSE do HistGradientBoosting: 0.3457\n",
      "RMSE do MLP: 0.8605\n",
      "RMSE do Modelo Meta: 0.3292\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nKNeighborsRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRMSE do Modelo Meta: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse_meta\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Obter previs√µes dos modelos base no conjunto de teste\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m predicoes_teste \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcolumn_stack([modelo\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;28;01mfor\u001b[39;00m modelo \u001b[38;5;129;01min\u001b[39;00m modelos])\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Criar novo conjunto de teste para o modelo meta\u001b[39;00m\n\u001b[1;32m     77\u001b[0m X_meta_test \u001b[38;5;241m=\u001b[39m predicoes_teste\n",
      "Cell \u001b[0;32mIn[9], line 74\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRMSE do Modelo Meta: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse_meta\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Obter previs√µes dos modelos base no conjunto de teste\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m predicoes_teste \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcolumn_stack([modelo\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;28;01mfor\u001b[39;00m modelo \u001b[38;5;129;01min\u001b[39;00m modelos])\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Criar novo conjunto de teste para o modelo meta\u001b[39;00m\n\u001b[1;32m     77\u001b[0m X_meta_test \u001b[38;5;241m=\u001b[39m predicoes_teste\n",
      "File \u001b[0;32m~/Arquivos/conda/envs/ML/lib/python3.11/site-packages/sklearn/neighbors/_regression.py:243\u001b[0m, in \u001b[0;36mKNeighborsRegressor.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict the target for the provided data.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m    Target values.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# In that case, we do not need the distances to perform\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m# the weighting so we do not compute them.\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m     neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkneighbors(X, return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    244\u001b[0m     neigh_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Arquivos/conda/envs/ML/lib/python3.11/site-packages/sklearn/neighbors/_base.py:838\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    836\u001b[0m         X \u001b[38;5;241m=\u001b[39m _check_precomputed(X)\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 838\u001b[0m         X \u001b[38;5;241m=\u001b[39m validate_data(\n\u001b[1;32m    839\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    840\u001b[0m             X,\n\u001b[1;32m    841\u001b[0m             ensure_all_finite\u001b[38;5;241m=\u001b[39mensure_all_finite,\n\u001b[1;32m    842\u001b[0m             accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    843\u001b[0m             reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    844\u001b[0m             order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    845\u001b[0m         )\n\u001b[1;32m    847\u001b[0m n_samples_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_fit_\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_neighbors \u001b[38;5;241m>\u001b[39m n_samples_fit:\n",
      "File \u001b[0;32m~/Arquivos/conda/envs/ML/lib/python3.11/site-packages/sklearn/utils/validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/Arquivos/conda/envs/ML/lib/python3.11/site-packages/sklearn/utils/validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[0;32m-> 1107\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1108\u001b[0m         array,\n\u001b[1;32m   1109\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m   1110\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m   1111\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mensure_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1112\u001b[0m     )\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/Arquivos/conda/envs/ML/lib/python3.11/site-packages/sklearn/utils/validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    121\u001b[0m     X,\n\u001b[1;32m    122\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[1;32m    123\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[1;32m    124\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[1;32m    125\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    126\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    127\u001b[0m )\n",
      "File \u001b[0;32m~/Arquivos/conda/envs/ML/lib/python3.11/site-packages/sklearn/utils/validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nKNeighborsRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Carregar datasets\n",
    "data_treino = pd.read_csv('/home/caio/github/k-3/data/train.csv')\n",
    "data_teste = pd.read_csv('/home/caio/github/k-3/data/test.csv')\n",
    "\n",
    "# Definir features e target\n",
    "X = data_treino.drop(columns=['rainfall'])  # Substitua pelo nome real da coluna alvo\n",
    "y = data_treino['rainfall']\n",
    "X_test = data_teste  # Apenas features\n",
    "\n",
    "# Dividir treino em treino principal (60%) e valida√ß√£o (40%)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Inicializar modelos base\n",
    "modelo_catboost = CatBoostRegressor(verbose=0, random_state=42)\n",
    "modelo_xgboost = XGBRegressor(n_estimators=100, random_state=42)\n",
    "modelo_lightgbm = LGBMRegressor(n_estimators=100, random_state=42)\n",
    "modelo_knn = KNeighborsRegressor(n_neighbors=5)\n",
    "modelo_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "modelo_extra_trees = ExtraTreesRegressor(n_estimators=100, random_state=42)\n",
    "modelo_gb = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "modelo_dt = DecisionTreeRegressor(random_state=42)\n",
    "modelo_hgb = HistGradientBoostingRegressor(random_state=42)\n",
    "modelo_mlp = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "\n",
    "# Treinar modelos base no treino principal\n",
    "modelos = [\n",
    "    modelo_catboost, modelo_xgboost, modelo_lightgbm, modelo_knn, \n",
    "    modelo_rf, modelo_extra_trees, modelo_gb, modelo_dt, modelo_hgb, modelo_mlp\n",
    "]\n",
    "\n",
    "for modelo in modelos:\n",
    "    modelo.fit(X_train, y_train)\n",
    "\n",
    "# Obter previs√µes dos modelos base no conjunto de valida√ß√£o\n",
    "predicoes_valid = np.column_stack([modelo.predict(X_valid) for modelo in modelos])\n",
    "\n",
    "# Calcular RMSE para cada modelo individual\n",
    "rmse_individuais = [np.sqrt(mean_squared_error(y_valid, pred)) for pred in predicoes_valid.T]\n",
    "\n",
    "# Criar novo conjunto de treino para o modelo meta\n",
    "X_meta_train = predicoes_valid\n",
    "y_meta_train = y_valid\n",
    "\n",
    "# Treinar modelo meta (Regress√£o Ridge)\n",
    "modelo_meta = Ridge()\n",
    "modelo_meta.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Fazer previs√µes no conjunto de valida√ß√£o usando o modelo meta\n",
    "y_valid_pred = modelo_meta.predict(X_meta_train)\n",
    "\n",
    "# Calcular RMSE do modelo meta\n",
    "rmse_meta = np.sqrt(mean_squared_error(y_meta_train, y_valid_pred))\n",
    "\n",
    "# Exibir RMSEs\n",
    "nomes_modelos = [\"CatBoost\", \"XGBoost\", \"LightGBM\", \"KNN\", \"RandomForest\", \"ExtraTrees\", \"GradientBoosting\", \"DecisionTree\", \"HistGradientBoosting\", \"MLP\"]\n",
    "for nome, rmse in zip(nomes_modelos, rmse_individuais):\n",
    "    print(f\"RMSE do {nome}: {rmse:.4f}\")\n",
    "print(f\"RMSE do Modelo Meta: {rmse_meta:.4f}\")\n",
    "\n",
    "# Obter previs√µes dos modelos base no conjunto de teste\n",
    "predicoes_teste = np.column_stack([modelo.predict(X_test) for modelo in modelos])\n",
    "\n",
    "# Criar novo conjunto de teste para o modelo meta\n",
    "X_meta_test = predicoes_teste\n",
    "\n",
    "# Fazer previs√£o final com o modelo meta\n",
    "y_pred_final = modelo_meta.predict(X_meta_test)\n",
    "\n",
    "# Salvar resultados\n",
    "output = pd.DataFrame({'Id': data_teste['id'], 'rainfall': y_pred_final})\n",
    "output.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Blending conclu√≠do! Resultados salvos em submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf8bb20f-b183-4df8-ab95-c88d4537b8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando CatBoost...\n",
      "Treinando XGBoost...\n",
      "Treinando LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000083 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1350\n",
      "[LightGBM] [Info] Number of data points in the train set: 1314, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 0.751142\n",
      "Treinando RandomForest...\n",
      "Treinando ExtraTrees...\n",
      "Treinando DecisionTree...\n",
      "Treinando HistGradientBoosting...\n",
      "Treinando MLP...\n",
      "Treinando AdaBoost...\n",
      "Treinando Bagging...\n",
      "Treinando Lasso...\n",
      "Treinando ElasticNet...\n",
      "Treinando BayesianRidge...\n",
      "Treinando SVR...\n",
      "Treinando GaussianProcess...\n",
      "Treinando HuberRegressor...\n",
      "Treinando PassiveAggressive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caio/Arquivos/conda/envs/ML/lib/python3.11/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE do CatBoost: 0.3446\n",
      "RMSE do XGBoost: 0.3607\n",
      "RMSE do LightGBM: 0.3455\n",
      "RMSE do RandomForest: 0.3398\n",
      "RMSE do ExtraTrees: 0.3355\n",
      "RMSE do DecisionTree: 0.4790\n",
      "RMSE do HistGradientBoosting: 0.3484\n",
      "RMSE do MLP: 0.6616\n",
      "RMSE do AdaBoost: 0.3476\n",
      "RMSE do Bagging: 0.3405\n",
      "RMSE do Lasso: 0.3334\n",
      "RMSE do ElasticNet: 0.3313\n",
      "RMSE do BayesianRidge: 0.3314\n",
      "RMSE do SVR: 0.3382\n",
      "RMSE do GaussianProcess: 0.8700\n",
      "RMSE do HuberRegressor: 0.3699\n",
      "RMSE do PassiveAggressive: 0.3711\n",
      "\n",
      "üî• RMSE do Modelo Meta (XGBoost): 0.0367\n",
      "Blending conclu√≠do! Resultados salvos em submission.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, \n",
    "    HistGradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import (\n",
    "    Lasso, ElasticNet, BayesianRidge, HuberRegressor, PassiveAggressiveRegressor, SGDRegressor\n",
    ")\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Carregar datasets\n",
    "data_treino = pd.read_csv('/home/caio/github/k-3/data/train_tratado.csv')\n",
    "data_teste = pd.read_csv('/home/caio/github/k-3/data/teste_tratado.csv')\n",
    "\n",
    "# Definir features e target\n",
    "X = data_treino.drop(columns=['rainfall'])  # Substitua pelo nome real da coluna alvo\n",
    "y = data_treino['rainfall']\n",
    "X_test = data_teste  # Apenas features\n",
    "\n",
    "# Dividir treino em treino principal (60%) e valida√ß√£o (40%)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Inicializar modelos base\n",
    "modelos = {\n",
    "    \"CatBoost\": CatBoostRegressor(verbose=0, random_state=42),\n",
    "    \"XGBoost\":n_estimators=100, random_state=42\n",
    "    \"LightGBM\": LGBMRegressor(n_estimators=100, random_state=42),\n",
    "    #\"KNN\": KNeighborsRegressor(n_neighbors=5),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesRegressor(n_estimators=100, random_state=42),\n",
    "    #\"GradientBoosting\": GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    \"DecisionTree\": DecisionTreeRegressor(random_state=42),\n",
    "    \"HistGradientBoosting\": HistGradientBoostingRegressor(random_state=42),\n",
    "    \"MLP\": MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42),\n",
    "    \"AdaBoost\": AdaBoostRegressor(n_estimators=100, random_state=42),\n",
    "    \"Bagging\": BaggingRegressor(n_estimators=100, random_state=42),\n",
    "    \"Lasso\": Lasso(alpha=0.1),\n",
    "    \"ElasticNet\": ElasticNet(alpha=0.1),\n",
    "    \"BayesianRidge\": BayesianRidge(),\n",
    "    \"SVR\": SVR(kernel='rbf', C=100),\n",
    "    \"GaussianProcess\": GaussianProcessRegressor(),\n",
    "    \"HuberRegressor\": HuberRegressor(),\n",
    "    \"PassiveAggressive\": PassiveAggressiveRegressor(max_iter=1000, random_state=42),\n",
    "    #\"SGDRegressor\": SGDRegressor(max_iter=1000, random_state=42),\n",
    "}\n",
    "\n",
    "# Treinar modelos base no treino principal\n",
    "for nome, modelo in modelos.items():\n",
    "    print(f\"Treinando {nome}...\")\n",
    "    modelo.fit(X_train, y_train)\n",
    "\n",
    "# Obter previs√µes dos modelos base no conjunto de valida√ß√£o\n",
    "predicoes_valid = np.column_stack([modelo.predict(X_valid) for modelo in modelos.values()])\n",
    "\n",
    "# Calcular RMSE para cada modelo individual\n",
    "rmse_individuais = {nome: np.sqrt(mean_squared_error(y_valid, pred)) \n",
    "                     for nome, pred in zip(modelos.keys(), predicoes_valid.T)}\n",
    "\n",
    "# Criar novo conjunto de treino para o modelo meta\n",
    "X_meta_train = predicoes_valid\n",
    "y_meta_train = y_valid\n",
    "\n",
    "# **NOVO META-MODELO: XGBoostRegressor**\n",
    "modelo_meta = XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=5, random_state=42)\n",
    "modelo_meta.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Fazer previs√µes no conjunto de valida√ß√£o usando o modelo meta\n",
    "y_valid_pred = modelo_meta.predict(X_meta_train)\n",
    "\n",
    "# Calcular RMSE do modelo meta\n",
    "rmse_meta = np.sqrt(mean_squared_error(y_meta_train, y_valid_pred))\n",
    "\n",
    "# Exibir RMSEs\n",
    "for nome, rmse in rmse_individuais.items():\n",
    "    print(f\"RMSE do {nome}: {rmse:.4f}\")\n",
    "print(f\"\\nüî• RMSE do Modelo Meta (XGBoost): {rmse_meta:.4f}\")\n",
    "\n",
    "# Obter previs√µes dos modelos base no conjunto de teste\n",
    "predicoes_teste = np.column_stack([modelo.predict(X_test) for modelo in modelos.values()])\n",
    "\n",
    "# Criar novo conjunto de teste para o modelo meta\n",
    "X_meta_test = predicoes_teste\n",
    "\n",
    "# Fazer previs√£o final com o modelo meta\n",
    "y_pred_final = modelo_meta.predict(X_meta_test)\n",
    "\n",
    "# Salvar resultados\n",
    "output = pd.DataFrame({'Id': data_teste['id'], 'rainfall': y_pred_final})\n",
    "output.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Blending conclu√≠do! Resultados salvos em submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b692c10-2f6b-4420-a46b-1a60d010da5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando CatBoost...\n",
      "Treinando XGBoost...\n",
      "Treinando LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000091 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1316\n",
      "[LightGBM] [Info] Number of data points in the train set: 1095, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 0.749772\n",
      "Treinando RandomForest...\n",
      "Treinando ExtraTrees...\n",
      "Treinando DecisionTree...\n",
      "Treinando HistGradientBoosting...\n",
      "Treinando MLP...\n",
      "Treinando AdaBoost...\n",
      "Treinando Bagging...\n",
      "Treinando Lasso...\n",
      "Treinando ElasticNet...\n",
      "Treinando BayesianRidge...\n",
      "Treinando SVR...\n",
      "Treinando GaussianProcess...\n",
      "\n",
      "‚úÖ RMSE Real do Modelo Meta: 0.3585\n",
      "Blending conclu√≠do! Resultados salvos em submission.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor, \n",
    "    AdaBoostRegressor, BaggingRegressor\n",
    ")\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import Lasso, ElasticNet, BayesianRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Carregar datasets\n",
    "data_treino = pd.read_csv('/home/caio/github/k-3/data/train_tratado.csv')\n",
    "data_teste = pd.read_csv('/home/caio/github/k-3/data/teste_tratado.csv')\n",
    "\n",
    "# Definir features e target\n",
    "X = data_treino.drop(columns=['rainfall'])  # Substitua pelo nome real da coluna alvo\n",
    "y = data_treino['rainfall']\n",
    "X_test = data_teste\n",
    "\n",
    "# Novo Split: Treino (50%), Valida√ß√£o para Base (30%), Teste para Meta (20%)\n",
    "X_train_base, X_rest, y_train_base, y_rest = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "X_valid_base, X_test_meta, y_valid_base, y_test_meta = train_test_split(X_rest, y_rest, test_size=0.4, random_state=42)\n",
    "\n",
    "# Inicializar modelos base\n",
    "modelos = {\n",
    "    \"CatBoost\": CatBoostRegressor(verbose=0, random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, random_state=42),\n",
    "    \"LightGBM\": LGBMRegressor(n_estimators=100, random_state=42),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesRegressor(n_estimators=100, random_state=42),\n",
    "    \"DecisionTree\": DecisionTreeRegressor(random_state=42),\n",
    "    \"HistGradientBoosting\": HistGradientBoostingRegressor(random_state=42),\n",
    "    \"MLP\": MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42),\n",
    "    \"AdaBoost\": AdaBoostRegressor(n_estimators=100, random_state=42),\n",
    "    \"Bagging\": BaggingRegressor(n_estimators=100, random_state=42),\n",
    "    \"Lasso\": Lasso(alpha=0.1),\n",
    "    \"ElasticNet\": ElasticNet(alpha=0.1),\n",
    "    \"BayesianRidge\": BayesianRidge(),\n",
    "    \"SVR\": SVR(kernel='rbf', C=100),\n",
    "    \"GaussianProcess\": GaussianProcessRegressor()\n",
    "}\n",
    "\n",
    "# Treinar modelos base no treino principal\n",
    "for nome, modelo in modelos.items():\n",
    "    print(f\"Treinando {nome}...\")\n",
    "    modelo.fit(X_train_base, y_train_base)\n",
    "\n",
    "# Obter previs√µes dos Modelos Base na Valida√ß√£o Base (para treinar o meta-modelo)\n",
    "predicoes_valid_base = np.column_stack([modelo.predict(X_valid_base) for modelo in modelos.values()])\n",
    "\n",
    "# Criar e treinar o Meta-Modelo\n",
    "modelo_meta = XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=5, random_state=42)\n",
    "modelo_meta.fit(predicoes_valid_base, y_valid_base)\n",
    "\n",
    "# Avaliar o Meta-Modelo num conjunto nunca visto (Teste Meta)\n",
    "predicoes_test_meta = np.column_stack([modelo.predict(X_test_meta) for modelo in modelos.values()])\n",
    "y_pred_meta = modelo_meta.predict(predicoes_test_meta)\n",
    "\n",
    "# Calcular RMSE real do modelo meta\n",
    "rmse_meta_real = np.sqrt(mean_squared_error(y_test_meta, y_pred_meta))\n",
    "print(f\"\\n‚úÖ RMSE Real do Modelo Meta: {rmse_meta_real:.4f}\")\n",
    "\n",
    "# Obter previs√µes dos modelos base no conjunto de teste\n",
    "predicoes_teste = np.column_stack([modelo.predict(X_test) for modelo in modelos.values()])\n",
    "\n",
    "# Fazer previs√£o final com o modelo meta\n",
    "y_pred_final = modelo_meta.predict(predicoes_teste)\n",
    "\n",
    "# Salvar resultados\n",
    "output = pd.DataFrame({'Id': data_teste['id'], 'rainfall': y_pred_final})\n",
    "output.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Blending conclu√≠do! Resultados salvos em submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19454c79-03ed-4ec3-bc1e-a7bdb4288388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "# Definir o modelo\n",
    "xgb = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Definir os hiperpar√¢metros para a busca\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(100, 7000, 10),  # Aumentado at√© 7000 e passo reduzido para testar mais varia√ß√µes\n",
    "    'learning_rate': np.linspace(0.00005, 0.8, 100),  # 100 valores para mais refinamento da taxa de aprendizado\n",
    "    'max_depth': np.arange(2, 30),  # Aumentado at√© 30 para testar √°rvores ainda mais profundas\n",
    "    'subsample': np.linspace(0.1, 1, 30),  # Mais valores para melhor ajuste da fra√ß√£o de amostras\n",
    "    'colsample_bytree': np.linspace(0.1, 1, 30)  # Mais valores para ajuste fino da fra√ß√£o de features\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Criar a busca com RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,  # N√∫mero de combina√ß√µes a testar\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Treinar a busca\n",
    "random_search.fit(feature_treino_base, target_treino_base)\n",
    "\n",
    "# Exibir apenas os melhores hiperpar√¢metros\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdb69c7-9b21-4ae3-a586-8bda4422a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from catboost import CatBoostRegressor\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Defini√ß√£o da fun√ß√£o objetivo para otimiza√ß√£o\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 100, 3000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 1.0, log=True),\n",
    "        'depth': trial.suggest_int('depth', 2, 8),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.2, 1.0),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.0, 20.0),\n",
    "        'random_seed': 42,\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    # Criar modelo CatBoostRegressor com os hiperpar√¢metros sugeridos\n",
    "    model = CatBoostRegressor(**params)\n",
    "\n",
    "    # Avalia√ß√£o do modelo com valida√ß√£o cruzada\n",
    "    scores = cross_val_score(model, feature_treino_base, target_treino_base, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    \n",
    "    return np.mean(scores)  # M√©dia do erro quadr√°tico negativo\n",
    "\n",
    "# Criando um estudo e rodando a otimiza√ß√£o\n",
    "study = optuna.create_study(direction='maximize')  # Maximizar -MSE (equivalente a minimizar MSE)\n",
    "study.optimize(objective, n_trials=50)  # Ajuste o n√∫mero de trials\n",
    "\n",
    "# Exibir os melhores hiperpar√¢metros encontrados\n",
    "print(\"Melhores hiperpar√¢metros:\", study.best_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
